{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// import $ivy.`net.tixxit::framian:0.5.0`\n",
    "// import framian.csv.Csv\n",
    "// import java.io.File\n",
    "// case class Country(name: String, code: String, code2: String, numeric: Int)\n",
    "// Csv.parseFile(new File(\"../../data/countries.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36msparkSession\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@704c80a"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-2.6`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.apache.hadoop:hadoop-aws:2.6.4`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._\n",
    "\n",
    "val sparkSession = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "  .jupyter() // this method must be called straightaway after builder()\n",
    "  // .yarn(\"/etc/hadoop/conf\") // optional, for Spark on YARN - argument is the Hadoop conf directory\n",
    "  // .emr(\"2.6.4\") // on AWS ElasticMapReduce, this adds aws-related to the spark jar list\n",
    "  .master(\"local\") // change to \"yarn-client\" on YARN\n",
    "  // .config(\"spark.executor.instances\", \"10\")\n",
    "  // .config(\"spark.executor.memory\", \"3g\")\n",
    "  // .config(\"spark.hadoop.fs.s3a.access.key\", awsCredentials._1)\n",
    "  // .config(\"spark.hadoop.fs.s3a.secret.key\", awsCredentials._2)\n",
    "  .appName(\"notebook\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msparkSession.implicits._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparkSession.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [index: string, code: string ... 3 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = sparkSession.read.load(\"../../data/countries.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling domain.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$     , domain._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.domain, domain._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: string (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      " |-- code2: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- numeric: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: Unable to generate an encoder for inner class `$file.$domainWrapper$Helper$Country` without access to the scope that this class was defined in.",
      "Try moving this class out of its parent class.;\u001b[39m",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNewInstance$$anonfun$apply$33$$anonfun$applyOrElse$13.applyOrElse(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m2104\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNewInstance$$anonfun$apply$33$$anonfun$applyOrElse$13.applyOrElse(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m2100\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m288\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m288\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m70\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m287\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionDown$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m248\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m258\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m267\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m188\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m267\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m236\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNewInstance$$anonfun$apply$33.applyOrElse(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m2100\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNewInstance$$anonfun$apply$33.applyOrElse(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m2096\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m61\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m61\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m70\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m60\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNewInstance$.apply(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m2096\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNewInstance$.apply(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m2095\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m85\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m82\u001b[39m)",
      "  scala.collection.LinearSeqOptimized$class.foldLeft(\u001b[32mLinearSeqOptimized.scala\u001b[39m:\u001b[32m124\u001b[39m)",
      "  scala.collection.immutable.List.foldLeft(\u001b[32mList.scala\u001b[39m:\u001b[32m84\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m82\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m74\u001b[39m)",
      "  scala.collection.immutable.List.foreach(\u001b[32mList.scala\u001b[39m:\u001b[32m381\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m74\u001b[39m)",
      "  org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.resolveAndBind(\u001b[32mExpressionEncoder.scala\u001b[39m:\u001b[32m258\u001b[39m)",
      "  org.apache.spark.sql.Dataset.<init>(\u001b[32mDataset.scala\u001b[39m:\u001b[32m209\u001b[39m)",
      "  org.apache.spark.sql.Dataset.<init>(\u001b[32mDataset.scala\u001b[39m:\u001b[32m167\u001b[39m)",
      "  org.apache.spark.sql.Dataset$.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m58\u001b[39m)",
      "  org.apache.spark.sql.Dataset.as(\u001b[32mDataset.scala\u001b[39m:\u001b[32m376\u001b[39m)",
      "  $sess.cmd5Wrapper$Helper.<init>(\u001b[32mcmd5.sc\u001b[39m:\u001b[32m1\u001b[39m)",
      "  $sess.cmd5Wrapper.<init>(\u001b[32mcmd5.sc\u001b[39m:\u001b[32m320\u001b[39m)",
      "  $sess.cmd5$.<init>(\u001b[32mcmd5.sc\u001b[39m:\u001b[32m254\u001b[39m)",
      "  $sess.cmd5$.<clinit>(\u001b[32mcmd5.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "df.as[Country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
